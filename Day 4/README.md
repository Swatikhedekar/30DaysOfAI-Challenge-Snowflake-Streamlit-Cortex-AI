# ‚ö° Day 4: Caching Your App
### 30 Days of AI ‚Äì Streamlit √ó Snowflake Cortex

This project is part of the **30DaysOfAI-challenge**, designed to help you build AI applications using **Streamlit** and **Snowflake Cortex**.

On **Day 4**, we focus on improving performance by **caching LLM responses** in a Streamlit app that calls a Snowflake Cortex model.

---

## üß† What You‚Äôll Learn
- How to call a Cortex LLM from Streamlit
- How to use `st.cache_data` to cache AI responses
- How caching reduces response time for repeated prompts
- How to measure and display request execution time

---

## üõ†Ô∏è Tech Stack
- Streamlit  
- Snowflake Snowpark  
- Snowflake Cortex (LLMs)

---

## ‚öôÔ∏è How It Works
1. User enters a prompt in the app
2. The prompt is sent to a Cortex LLM (e.g., Claude 3.5 Sonnet)
3. The response is cached using `st.cache_data`
4. Repeated prompts return instantly from cache
5. Execution time is displayed along with the response

---

## ‚ñ∂Ô∏è Output
- AI-generated response displayed in the app  
- Time taken for the request shown clearly  
- Cached responses load almost instantly

---

## üìö Resources
- Streamlit `st.cache_data` Documentation  
- Caching in Streamlit  
- Streamlit in Snowflake Caching Limitations  

---

‚ú® *Day 4 complete ‚Äî faster AI apps with smart caching!*

